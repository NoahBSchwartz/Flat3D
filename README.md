# 3D-Screen-Illusion
This is code that was created to compete in the TSA Software Development Competition. I saw an article about Google's new screen that made video look 3D by tracking the user's face and moving the video based on their head movements. I thought it might be possible to get this same result on a normal laptop by using the Mediapipe face tracking code (https://google.github.io/mediapipe/solutions/face_mesh.html) to move a Blender scene based on where I was in the frame. The challenging part of the project was getting the Mediapipe code to interact with Blender at a convincing frame rate. Because they both relied on different Python versions, I ended up using socket communication to send the data to Blender which then and angled the camera.  
![resized_converted_video](https://github.com/NoahBSchwartz/3D-Screen-Illusion/assets/44248582/c748cc65-f71d-48bc-a94f-96bf1c9f52e6)
